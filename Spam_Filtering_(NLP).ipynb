{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Original Code for Extremely Long Processing**"
      ],
      "metadata": {
        "id": "NHWUhhlZVHHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OVkHCOiYxroS",
        "outputId": "87d107fd-c883-4b1b-da11-e8ded90dc74a",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting trl\n",
            "  Downloading trl-0.16.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from trl) (1.5.2)\n",
            "Collecting datasets>=3.0.0 (from trl)\n",
            "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (13.9.4)\n",
            "Requirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.11/dist-packages (from trl) (4.50.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (0.29.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=3.0.0->trl)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
            "Collecting xxhash (from datasets>=3.0.0->trl)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=3.0.0->trl)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.11.14)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl) (0.21.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.18.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.12.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.34.0->trl) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl) (3.0.2)\n",
            "Downloading trl-0.16.0-py3-none-any.whl (335 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.7/335.7 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, trl\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.4.1 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 trl-0.16.0 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              },
              "id": "cfb558027f0c4d02b500f93f09c387d5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "JVoIYPgNxLqU",
        "outputId": "e3619e4f-0924-44ca-9c30-d35bb4f79b3e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Loading and preprocessing data...\n",
            "Training on 1600 examples, testing on 400 examples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training universal spam classifier...\n",
            "Epoch 1/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 24%|██▍       | 392/1600 [18:54<58:17,  2.90s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b8873a234cc6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-b8873a234cc6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training universal spam classifier...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;31m# Plot learning curves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-b8873a234cc6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, eval_freq)\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36m_no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_no_grad_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_no_grad_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_if_nonfinite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforeach\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m     \u001b[0m_clip_grads_with_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforeach\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36m_no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_no_grad_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_no_grad_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36m_get_total_norm\u001b[0;34m(tensors, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mforeach\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_device_has_foreach_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         ):\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mnorms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mforeach\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             raise RuntimeError(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "from trl import GRPOConfig\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Set seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize RoBERTa model and tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "class UniversalSpamClassifier(nn.Module):\n",
        "    def __init__(self, hidden_size=768):\n",
        "        super(UniversalSpamClassifier, self).__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
        "        self.classifier = nn.Linear(hidden_size, 2)  # Binary classification\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n",
        "def preprocess_text(text, is_sms=False):\n",
        "    \"\"\"Universal text preprocessing for both SMS and email\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    # Convert to string if not already\n",
        "    text = str(text)\n",
        "\n",
        "    # Common preprocessing for both SMS and email\n",
        "    text = text.lower()\n",
        "\n",
        "    # Specific preprocessing for SMS\n",
        "    if is_sms:\n",
        "        # Replace email addresses\n",
        "        text = re.sub(r'\\S+@\\S+', 'emailaddr', text)\n",
        "        # Replace URLs\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', 'webaddress', text, flags=re.MULTILINE)\n",
        "        # Replace money symbols\n",
        "        text = re.sub(r'£|\\$', 'moneysymb', text)\n",
        "        # Replace phone numbers\n",
        "        text = re.sub(r'[\\+\\d\\-\\s]{10,}', 'phonenum', text)\n",
        "        # Replace numbers\n",
        "        text = re.sub(r'\\d+(\\.\\d+)?', 'num', text)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    words = [w for w in words if w not in stop_words]\n",
        "\n",
        "    # Stemming\n",
        "    ps = PorterStemmer()\n",
        "    words = [ps.stem(w) for w in words]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "def load_and_preprocess_data(file_path, is_sms=False, sample_size=None):\n",
        "    \"\"\"Load and preprocess data from CSV file\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    if sample_size:\n",
        "        df = df.sample(n=min(sample_size, len(df)), random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # Handle different CSV formats\n",
        "    if 'label_num' in df.columns:\n",
        "        labels = df['label_num'].tolist()\n",
        "    elif 'v1' in df.columns:  # SMS spam dataset format\n",
        "        labels = (df['v1'] == 'spam').astype(int).tolist()\n",
        "    else:\n",
        "        raise ValueError(\"Could not determine label column in dataset\")\n",
        "\n",
        "    # Get text column\n",
        "    if 'text' in df.columns:\n",
        "        texts = df['text'].tolist()\n",
        "    elif 'v2' in df.columns:  # SMS spam dataset format\n",
        "        texts = df['v2'].tolist()\n",
        "    else:\n",
        "        raise ValueError(\"Could not determine text column in dataset\")\n",
        "\n",
        "    # Preprocess texts\n",
        "    preprocessed_texts = [preprocess_text(text, is_sms) for text in texts]\n",
        "\n",
        "    # Split into training and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        preprocessed_texts, labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "class SpamEnvironment:\n",
        "    def __init__(self, model, tokenizer, texts, labels):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.current_idx = 0\n",
        "\n",
        "    def step(self, action):\n",
        "        true_label = self.labels[self.current_idx]\n",
        "\n",
        "        if action.argmax().item() == true_label:\n",
        "            reward = 1.0\n",
        "        else:\n",
        "            reward = -1.0\n",
        "\n",
        "        self.current_idx = (self.current_idx + 1) % len(self.texts)\n",
        "        return torch.tensor([reward], device=device)\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_idx = random.randint(0, len(self.texts) - 1)\n",
        "        return self.get_current_input()\n",
        "\n",
        "    def get_current_input(self):\n",
        "        text = self.texts[self.current_idx]\n",
        "        encoding = self.tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=128)\n",
        "        return {k: v.to(device) for k, v in encoding.items()}\n",
        "\n",
        "class UniversalSpamTrainer:\n",
        "    def __init__(self, model, tokenizer, train_texts, train_labels, test_texts, test_labels):\n",
        "        self.model = model.to(device)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.train_env = SpamEnvironment(model, tokenizer, train_texts, train_labels)\n",
        "        self.test_env = SpamEnvironment(model, tokenizer, test_texts, test_labels)\n",
        "\n",
        "        self.ppo_config = GRPOConfig(\n",
        "            learning_rate=1e-5,\n",
        "            gradient_accumulation_steps=1,\n",
        "            seed=42,\n",
        "            output_dir='./'\n",
        "        )\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.ppo_config.learning_rate)\n",
        "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'max', patience=2)\n",
        "        self.train_losses = []\n",
        "        self.accuracies = []\n",
        "        self.spam_probs = []\n",
        "\n",
        "    def train(self, epochs=10, eval_freq=1):\n",
        "        best_accuracy = 0.0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "            epoch_losses = []\n",
        "            epoch_spam_probs = []\n",
        "\n",
        "            for i in tqdm(range(len(self.train_env.texts))):\n",
        "                inputs = self.train_env.get_current_input()\n",
        "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "                logits = self.model(input_ids=inputs['input_ids'],\n",
        "                                   attention_mask=inputs['attention_mask'])\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                action = torch.multinomial(probs, 1)\n",
        "\n",
        "                reward = self.train_env.step(probs)\n",
        "\n",
        "                log_prob = torch.log(probs.gather(1, action))\n",
        "                loss = -log_prob * reward.to(device)\n",
        "                loss = loss.mean()\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
        "                self.optimizer.step()\n",
        "\n",
        "                epoch_losses.append(loss.item())\n",
        "                epoch_spam_probs.append(probs[0, 1].item())  # Probability of spam\n",
        "                self.train_env.current_idx = (self.train_env.current_idx + 1) % len(self.train_env.texts)\n",
        "\n",
        "            avg_loss = sum(epoch_losses) / len(epoch_losses) if epoch_losses else 0\n",
        "            self.train_losses.append(avg_loss)\n",
        "            avg_spam_prob = sum(epoch_spam_probs) / len(epoch_spam_probs) if epoch_spam_probs else 0\n",
        "            self.spam_probs.append(avg_spam_prob)\n",
        "\n",
        "            print(f\"Average training loss: {avg_loss:.4f}\")\n",
        "            print(f\"Average spam probability in training: {avg_spam_prob:.4f}\")\n",
        "\n",
        "            if (epoch + 1) % eval_freq == 0:\n",
        "                accuracy = self.evaluate()\n",
        "                self.accuracies.append(accuracy)\n",
        "                self.scheduler.step(accuracy)\n",
        "\n",
        "                if accuracy > best_accuracy:\n",
        "                    best_accuracy = accuracy\n",
        "                    torch.save(self.model.state_dict(), \"best_universal_spam_classifier.pt\")\n",
        "                    print(f\"Saved new best model with accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        self.model.load_state_dict(torch.load(\"best_universal_spam_classifier.pt\"))\n",
        "        return self.model\n",
        "\n",
        "    def evaluate(self):\n",
        "        self.model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        predictions = []\n",
        "        true_labels = []\n",
        "        spam_probs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in range(len(self.test_env.texts)):\n",
        "                text = self.test_env.texts[i]\n",
        "                label = self.test_env.labels[i]\n",
        "\n",
        "                inputs = self.tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=128)\n",
        "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "                logits = self.model(**inputs)\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                predicted = logits.argmax(-1).item()\n",
        "\n",
        "                correct += (predicted == label)\n",
        "                total += 1\n",
        "                predictions.append(predicted)\n",
        "                true_labels.append(label)\n",
        "                spam_probs.append(probs[0, 1].item())  # Probability of spam\n",
        "\n",
        "        accuracy = correct / total\n",
        "        print(f\"Test accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        # Generate classification report\n",
        "        report = classification_report(true_labels, predictions, target_names=['Not Spam', 'Spam'], digits=4)\n",
        "        print(\"\\n=== Classification Report ===\")\n",
        "        print(report)\n",
        "\n",
        "        # Create confusion matrix\n",
        "        cm = confusion_matrix(true_labels, predictions)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=['Not Spam', 'Spam'],\n",
        "                   yticklabels=['Not Spam', 'Spam'])\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.savefig('confusion_matrix_universal_spam.png')\n",
        "        plt.close()\n",
        "\n",
        "        # Plot spam probability distribution\n",
        "        self.plot_spam_distribution(spam_probs)\n",
        "\n",
        "        self.model.train()\n",
        "        return accuracy\n",
        "\n",
        "    def plot_spam_distribution(self, spam_probs):\n",
        "        \"\"\"Plot pie chart of spam vs not spam probabilities\"\"\"\n",
        "        avg_spam_prob = np.mean(spam_probs)\n",
        "        avg_ham_prob = 1 - avg_spam_prob\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.pie([avg_ham_prob, avg_spam_prob],\n",
        "                labels=['Not Spam', 'Spam'],\n",
        "                autopct='%1.1f%%',\n",
        "                colors=['lightgreen', 'lightcoral'],\n",
        "                startangle=90)\n",
        "        plt.title('Average Spam Probability Distribution')\n",
        "        plt.savefig('spam_distribution_pie.png')\n",
        "        plt.close()\n",
        "\n",
        "        # Create probability ranking\n",
        "        self.create_probability_ranking(spam_probs)\n",
        "\n",
        "    def create_probability_ranking(self, spam_probs):\n",
        "        \"\"\"Create and display a ranking of messages by spam probability\"\"\"\n",
        "        ranked_indices = np.argsort(spam_probs)[::-1]  # Sort descending\n",
        "        top_spam = ranked_indices[:5]\n",
        "        top_ham = ranked_indices[-5:][::-1]\n",
        "\n",
        "        print(\"\\n=== Top Spam Predictions ===\")\n",
        "        for i in top_spam:\n",
        "            text = self.test_env.texts[i][:100] + \"...\" if len(self.test_env.texts[i]) > 100 else self.test_env.texts[i]\n",
        "            print(f\"Prob: {spam_probs[i]:.4f} - {text}\")\n",
        "\n",
        "        print(\"\\n=== Top Not Spam Predictions ===\")\n",
        "        for i in top_ham:\n",
        "            text = self.test_env.texts[i][:100] + \"...\" if len(self.test_env.texts[i]) > 100 else self.test_env.texts[i]\n",
        "            print(f\"Prob: {spam_probs[i]:.4f} - {text}\")\n",
        "\n",
        "    def plot_learning_curves(self):\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        # Plot training loss\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.plot(self.train_losses)\n",
        "        plt.title('Training Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "\n",
        "        # Plot test accuracy\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.plot(self.accuracies)\n",
        "        plt.title('Test Accuracy')\n",
        "        plt.xlabel('Evaluation')\n",
        "        plt.ylabel('Accuracy')\n",
        "\n",
        "        # Plot spam probability trend\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.plot(self.spam_probs)\n",
        "        plt.title('Average Spam Probability')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Probability')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('learning_curves_universal_spam.png')\n",
        "        plt.close()\n",
        "\n",
        "    def predict(self, text, is_sms=False):\n",
        "        \"\"\"Predict whether text is spam or not\"\"\"\n",
        "        self.model.eval()\n",
        "        preprocessed_text = preprocess_text(text, is_sms)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = self.tokenizer(preprocessed_text, return_tensors='pt', truncation=True, padding='max_length', max_length=128)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            logits = self.model(**inputs)\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            prediction = logits.argmax(-1).item()\n",
        "            spam_prob = probs[0, 1].item()\n",
        "\n",
        "        self.model.train()\n",
        "        return {\n",
        "            \"prediction\": \"Spam\" if prediction == 1 else \"Not Spam\",\n",
        "            \"spam_probability\": spam_prob,\n",
        "            \"confidence\": max(probs[0, 0].item(), probs[0, 1].item())\n",
        "        }\n",
        "\n",
        "def main():\n",
        "    # You can use either email or SMS dataset here\n",
        "    email_path = \"/content/spam_ham_dataset.csv\"\n",
        "    sms_path = \"../content/spam.csv\"\n",
        "\n",
        "    print(\"Loading and preprocessing data...\")\n",
        "    # For email data\n",
        "    X_train, X_test, y_train, y_test = load_and_preprocess_data(email_path, is_sms=False, sample_size=2000)\n",
        "\n",
        "    # For SMS data (uncomment to use)\n",
        "    # X_train, X_test, y_train, y_test = load_and_preprocess_data(sms_path, is_sms=True, sample_size=2000)\n",
        "\n",
        "    print(f\"Training on {len(X_train)} examples, testing on {len(X_test)} examples\")\n",
        "\n",
        "    # Initialize model\n",
        "    model = UniversalSpamClassifier()\n",
        "\n",
        "    # Create trainer\n",
        "    trainer = UniversalSpamTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_texts=X_train,\n",
        "        train_labels=y_train,\n",
        "        test_texts=X_test,\n",
        "        test_labels=y_test\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    print(\"Training universal spam classifier...\")\n",
        "    trainer.train(epochs=1, eval_freq=1)\n",
        "\n",
        "    # Plot learning curves\n",
        "    trainer.plot_learning_curves()\n",
        "\n",
        "    # Display plots\n",
        "    import matplotlib.image as mpimg\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Learning curves\n",
        "    plt.subplot(2, 2, 1)\n",
        "    img = mpimg.imread('learning_curves_universal_spam.png')\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title('Learning Curves')\n",
        "\n",
        "    # Confusion matrix\n",
        "    plt.subplot(2, 2, 2)\n",
        "    img = mpimg.imread('confusion_matrix_universal_spam.png')\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title('Confusion Matrix')\n",
        "\n",
        "    # Spam distribution\n",
        "    plt.subplot(2, 2, 3)\n",
        "    img = mpimg.imread('spam_distribution_pie.png')\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title('Spam Distribution')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Test with example messages\n",
        "    test_messages = [\n",
        "        (\"Congratulations! You've won a free iPhone! Click here to claim now!\", True),\n",
        "        (\"Meeting tomorrow at 10 AM. Please prepare the quarterly report.\", False),\n",
        "        (\"URGENT: Your account has been compromised. Verify details now!\", True),\n",
        "        (\"Hi John, just checking in to see how you're doing.\", False),\n",
        "        (\"FREE entry to win £1000 prize! TEXT WIN to 12345 now!\", True),\n",
        "        (\"Your package will be delivered tomorrow between 2-4pm.\", False)\n",
        "    ]\n",
        "\n",
        "    print(\"\\n=== Testing Universal Spam Classifier ===\")\n",
        "    results = []\n",
        "    for msg, is_sms in test_messages:\n",
        "        result = trainer.predict(msg, is_sms)\n",
        "        results.append({\n",
        "            \"Message\": msg,\n",
        "            \"Type\": \"SMS\" if is_sms else \"Email\",\n",
        "            \"Prediction\": result[\"prediction\"],\n",
        "            \"Spam Probability\": f\"{result['spam_probability']:.4f}\",\n",
        "            \"Confidence\": f\"{result['confidence']:.4f}\"\n",
        "        })\n",
        "\n",
        "    # Display results as a table\n",
        "    results_df = pd.DataFrame(results)\n",
        "    print(\"\\nTest Message Results:\")\n",
        "    print(results_df.to_string(index=False))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Installing Dependencies & Importing Libraries**"
      ],
      "metadata": {
        "id": "bET2zBtBS0AP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell installs the required Python packages `(transformers, torch, pandas, and scikit-learn)` and imports them for use in the notebook. The transformers library provides the RoBERTa model and tokenizer, while torch is used for deep learning operations. pandas handles data loading and preprocessing, and scikit-learn is used for splitting the dataset into training and testing sets. The GradScaler and autocast from torch.cuda.amp enable mixed-precision training, which speeds up training on GPUs while maintaining accuracy."
      ],
      "metadata": {
        "id": "W7jLz2PtS24q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import time"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RANTCS4aR4hz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Device Configuration & Hyperparameters**"
      ],
      "metadata": {
        "id": "PWLsgKF7TAZz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell sets up the computing device `(GPU if available, otherwise CPU)` and configures TensorFloat-32 `(TF32)` for faster matrix operations on supported GPUs. The hyperparameters defined here control the model's behavior:\n",
        "\n",
        "MAX_LENGTH = 128: Truncates or pads input text to this length.\n",
        "\n",
        "BATCH_SIZE = 32: Number of samples processed per training step.\n",
        "\n",
        "EPOCHS = 3: Number of full passes through the dataset.\n",
        "\n",
        "SAMPLE_SIZE = 800: Limits the dataset size for faster experimentation.\n",
        "\n",
        "MODEL_NAME = \"distilroberta-base\": Uses a smaller, faster version of RoBERTa.\n"
      ],
      "metadata": {
        "id": "7RzFIaGuTCl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device.type == 'cuda':\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Hyperparameters\n",
        "MAX_LENGTH = 128\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "SAMPLE_SIZE = 800\n",
        "MODEL_NAME = \"distilroberta-base\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfVF4waGR6ME",
        "outputId": "f3b396e1-f3e3-482d-97d6-57e3c1773278"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Initialization & Optimizer Setup**"
      ],
      "metadata": {
        "id": "oJdMvsIuTP0z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the tokenizer and model are loaded from Hugging Face’s `transformers` library. The `RobertaForSequenceClassification` model is initialized with a new classification head (since it’s being fine-tuned for spam detection). The `ignore_mismatched_sizes=True` flag suppresses warnings about the new randomly initialized classifier layer. The AdamW optimizer is used with a learning rate of `5e-5`, which is standard for fine-tuning transformer models. The `GradScaler` is initialized to manage gradient scaling for mixed-precision training.\n",
        "\n"
      ],
      "metadata": {
        "id": "i9AIeUX5TQ0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== MODEL SETUP ==========\n",
        "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = RobertaForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=2,\n",
        "    ignore_mismatched_sizes=True\n",
        ").to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "scaler = GradScaler()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52y0wFKVR84d",
        "outputId": "144b4e23-a420-47b1-f97e-cc449975d5c0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-8-bf5c1c2d7f14>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Loading & Evaluation Functions**"
      ],
      "metadata": {
        "id": "1BaCoUOiTekB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines two key functions:\n",
        "\n",
        "**`load_data():`**\n",
        "\n",
        "1. Reads a CSV file (either SMS or email format).\n",
        "\n",
        "2. Handles encoding issues (falling back to latin1 if UTF-8 fails).\n",
        "\n",
        "3. Preprocesses text (lowercasing and truncation).\n",
        "\n",
        "4. Splits data into 80% training and 20% testing sets.\n",
        "\n",
        "**`evaluate():`**\n",
        "\n",
        "1. Computes model accuracy on test data.\n",
        "\n",
        "2. Uses larger batches (BATCH_SIZE * 2) for faster evaluation.\n",
        "\n",
        "3. Temporarily switches the model to evaluation mode (model.eval()) for inference."
      ],
      "metadata": {
        "id": "4C0MT_-FTgsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== DATA LOADING ==========\n",
        "def load_data(file_path, is_sms=False):\n",
        "    \"\"\"Load dataset with proper encoding handling\"\"\"\n",
        "    encoding = \"ISO-8859-1\" if is_sms else \"utf-8\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, encoding=encoding)\n",
        "    except UnicodeDecodeError:\n",
        "        df = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "    if is_sms:\n",
        "        df = df[['v1', 'v2']].sample(min(SAMPLE_SIZE, len(df)), random_state=42)\n",
        "        texts, labels = df['v2'].tolist(), (df['v1'] == 'spam').astype(int).tolist()\n",
        "    else:\n",
        "        df = df[['text', 'label_num']].sample(min(SAMPLE_SIZE, len(df)), random_state=42)\n",
        "        texts, labels = df['text'].tolist(), df['label_num'].tolist()\n",
        "\n",
        "    texts = [str(t).lower()[:500] for t in texts]\n",
        "    return train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# ========== EVALUATION FUNCTION ==========\n",
        "def evaluate(X_test, y_test):\n",
        "    model.eval()\n",
        "    test_encodings = tokenizer(X_test, padding=True, truncation=True,\n",
        "                             max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
        "    test_dataset = torch.utils.data.TensorDataset(\n",
        "        test_encodings['input_ids'].to(device),\n",
        "        test_encodings['attention_mask'].to(device),\n",
        "        torch.tensor(y_test).to(device)\n",
        "    )\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE*2)\n",
        "\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            correct += (outputs.logits.argmax(1) == labels).sum().item()\n",
        "\n",
        "    model.train()\n",
        "    return correct / len(y_test)"
      ],
      "metadata": {
        "id": "66mjeJ5JR-2i"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Loop & Execution**"
      ],
      "metadata": {
        "id": "0xrdxvhxSWNC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell contains the core training logic and executes the full workflow:\n",
        "\n",
        "**`train() Function:`**\n",
        "\n",
        "1. Tokenizes and batches training data.\n",
        "\n",
        "2. Uses mixed-precision training (autocast) on GPU for speed.\n",
        "\n",
        "3. Updates model weights using gradient scaling (GradScaler) if on GPU.\n",
        "\n",
        "4. Prints per-epoch metrics (loss, validation accuracy, and time).\n",
        "\n",
        "**`Main Execution:`**\n",
        "\n",
        "1. Loads the dataset (spam.csv).\n",
        "\n",
        "2. Starts training and times the process.\n",
        "\n",
        "3. Reports final accuracy on the test set.\n",
        "\n",
        "**`Expected Output:`**\n",
        "\n",
        "* Training progress for each epoch (loss and accuracy).\n",
        "\n",
        "* Total training time and final model performance."
      ],
      "metadata": {
        "id": "GvC1gqrpT-Uc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(X_train, y_train, X_test, y_test):\n",
        "    train_encodings = tokenizer(X_train, padding=True, truncation=True,\n",
        "                              max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
        "    train_dataset = torch.utils.data.TensorDataset(\n",
        "        train_encodings['input_ids'].to(device),\n",
        "        train_encodings['attention_mask'].to(device),\n",
        "        torch.tensor(y_train).to(device)\n",
        "    )\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids, attention_mask, labels = batch\n",
        "\n",
        "            with autocast(enabled=(device.type == 'cuda')):\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "\n",
        "            if device.type == 'cuda':\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        val_acc = evaluate(X_test[:200], y_test[:200])\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {epoch_loss/len(train_loader):.4f} | Val Acc: {val_acc:.4f} | Time: {time.time()-start_time:.2f}s\")\n",
        "\n",
        "# ========== MAIN EXECUTION ==========\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Loading data...\")\n",
        "    X_train, X_test, y_train, y_test = load_data(\"../content/spam.csv\", is_sms=True)\n",
        "\n",
        "    print(f\"\\nTraining on {len(X_train)} samples\")\n",
        "    print(\"Starting training...\")\n",
        "    train_start = time.time()\n",
        "    train(X_train, y_train, X_test, y_test)\n",
        "\n",
        "    final_acc = evaluate(X_test, y_test)\n",
        "    print(f\"\\nTraining completed in {time.time()-train_start:.2f} seconds\")\n",
        "    print(f\"Final Accuracy: {final_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7JJd2o5SFxP",
        "outputId": "ba2ce5ea-6113-4fbb-ba38-c06d2b34f728"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "\n",
            "Training on 640 samples\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-4fa2fcda39bf>:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device.type == 'cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 | Loss: 0.3159 | Val Acc: 0.9688 | Time: 2.42s\n",
            "Epoch 2/10 | Loss: 0.0348 | Val Acc: 0.9938 | Time: 2.45s\n",
            "Epoch 3/10 | Loss: 0.0360 | Val Acc: 0.9812 | Time: 2.45s\n",
            "Epoch 4/10 | Loss: 0.0265 | Val Acc: 0.9938 | Time: 2.41s\n",
            "Epoch 5/10 | Loss: 0.0049 | Val Acc: 0.9875 | Time: 2.40s\n",
            "Epoch 6/10 | Loss: 0.0011 | Val Acc: 0.9875 | Time: 2.51s\n",
            "Epoch 7/10 | Loss: 0.0008 | Val Acc: 0.9875 | Time: 2.63s\n",
            "Epoch 8/10 | Loss: 0.0004 | Val Acc: 0.9938 | Time: 2.46s\n",
            "Epoch 9/10 | Loss: 0.0006 | Val Acc: 0.9938 | Time: 2.62s\n",
            "Epoch 10/10 | Loss: 0.0003 | Val Acc: 0.9875 | Time: 2.61s\n",
            "\n",
            "Training completed in 25.68 seconds\n",
            "Final Accuracy: 0.9875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **COMPLETE CODE SEPARATELY!!!**"
      ],
      "metadata": {
        "id": "3cPWy3qSU5jG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import time\n",
        "\n",
        "# ========== Device Setup ==========\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device.type == 'cuda':\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ========== HYPERPARAMETERS ==========\n",
        "MAX_LENGTH = 128\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "SAMPLE_SIZE = 800\n",
        "MODEL_NAME = \"distilroberta-base\"\n",
        "\n",
        "# ========== MODEL SETUP ==========\n",
        "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = RobertaForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=2,\n",
        "    ignore_mismatched_sizes=True\n",
        ").to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "scaler = GradScaler()\n",
        "\n",
        "# ========== DATA LOADING ==========\n",
        "def load_data(file_path, is_sms=False):\n",
        "    \"\"\"Load dataset with proper encoding handling\"\"\"\n",
        "    encoding = \"ISO-8859-1\" if is_sms else \"utf-8\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, encoding=encoding)\n",
        "    except UnicodeDecodeError:\n",
        "        df = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "    if is_sms:\n",
        "        df = df[['v1', 'v2']].sample(min(SAMPLE_SIZE, len(df)), random_state=42)\n",
        "        texts, labels = df['v2'].tolist(), (df['v1'] == 'spam').astype(int).tolist()\n",
        "    else:\n",
        "        df = df[['text', 'label_num']].sample(min(SAMPLE_SIZE, len(df)), random_state=42)\n",
        "        texts, labels = df['text'].tolist(), df['label_num'].tolist()\n",
        "\n",
        "    texts = [str(t).lower()[:500] for t in texts]\n",
        "    return train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# ========== EVALUATION FUNCTION ==========\n",
        "def evaluate(X_test, y_test):\n",
        "    model.eval()\n",
        "    test_encodings = tokenizer(X_test, padding=True, truncation=True,\n",
        "                             max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
        "    test_dataset = torch.utils.data.TensorDataset(\n",
        "        test_encodings['input_ids'].to(device),\n",
        "        test_encodings['attention_mask'].to(device),\n",
        "        torch.tensor(y_test).to(device)\n",
        "    )\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE*2)\n",
        "\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            correct += (outputs.logits.argmax(1) == labels).sum().item()\n",
        "\n",
        "    model.train()\n",
        "    return correct / len(y_test)\n",
        "\n",
        "# ========== TRAINING LOOP ==========\n",
        "def train(X_train, y_train, X_test, y_test):\n",
        "    train_encodings = tokenizer(X_train, padding=True, truncation=True,\n",
        "                              max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
        "    train_dataset = torch.utils.data.TensorDataset(\n",
        "        train_encodings['input_ids'].to(device),\n",
        "        train_encodings['attention_mask'].to(device),\n",
        "        torch.tensor(y_train).to(device)\n",
        "    )\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids, attention_mask, labels = batch\n",
        "\n",
        "            with autocast(enabled=(device.type == 'cuda')):\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "\n",
        "            if device.type == 'cuda':\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        val_acc = evaluate(X_test[:200], y_test[:200])\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {epoch_loss/len(train_loader):.4f} | Val Acc: {val_acc:.4f} | Time: {time.time()-start_time:.2f}s\")\n",
        "\n",
        "# ========== MAIN EXECUTION ==========\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Loading data...\")\n",
        "    X_train, X_test, y_train, y_test = load_data(\"../content/spam.csv\", is_sms=True)\n",
        "\n",
        "    print(f\"\\nTraining on {len(X_train)} samples\")\n",
        "    print(\"Starting training...\")\n",
        "    train_start = time.time()\n",
        "    train(X_train, y_train, X_test, y_test)\n",
        "\n",
        "    final_acc = evaluate(X_test, y_test)\n",
        "    print(f\"\\nTraining completed in {time.time()-train_start:.2f} seconds\")\n",
        "    print(f\"Final Accuracy: {final_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BK2jrjzSMAS7",
        "outputId": "5fcef73b-385f-4c6b-c745-0f6be5b3d16e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-7-acb3e4e070cb>:30: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "\n",
            "Training on 640 samples\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-acb3e4e070cb>:93: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device.type == 'cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 | Loss: 0.2841 | Val Acc: 0.9750 | Time: 2.75s\n",
            "Epoch 2/10 | Loss: 0.0707 | Val Acc: 0.9938 | Time: 2.41s\n",
            "Epoch 3/10 | Loss: 0.0393 | Val Acc: 0.9938 | Time: 2.47s\n",
            "Epoch 4/10 | Loss: 0.0126 | Val Acc: 0.9938 | Time: 2.48s\n",
            "Epoch 5/10 | Loss: 0.0305 | Val Acc: 0.9875 | Time: 2.54s\n",
            "Epoch 6/10 | Loss: 0.0068 | Val Acc: 0.9875 | Time: 2.53s\n",
            "Epoch 7/10 | Loss: 0.0177 | Val Acc: 0.9875 | Time: 2.52s\n",
            "Epoch 8/10 | Loss: 0.0046 | Val Acc: 0.9812 | Time: 2.71s\n",
            "Epoch 9/10 | Loss: 0.0114 | Val Acc: 0.9812 | Time: 2.55s\n",
            "Epoch 10/10 | Loss: 0.0018 | Val Acc: 0.9938 | Time: 2.59s\n",
            "\n",
            "Training completed in 26.65 seconds\n",
            "Final Accuracy: 0.9938\n"
          ]
        }
      ]
    }
  ]
}